# -*- coding: utf-8 -*-
"""Copy of GIDY_PROJECT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_SeYb3y9t3r6iWRP-OvMDOxRa5AUYETL
"""

import zipfile
import os

# Path to uploaded ZIP file
dataset_zip = "/content/shop.v2i.yolov8.zip"  # Ensure correct path

# Extract location
extract_path = "/content/dataset"
os.makedirs(extract_path, exist_ok=True)

# Extract dataset
with zipfile.ZipFile(dataset_zip, "r") as zip_ref:
    zip_ref.extractall(extract_path)

print(f"✅ Dataset extracted to: {extract_path}")

import os
print(os.listdir(extract_path))

import cv2
import os

# Set paths
image_folder = "/content/dataset/train/images"  # Update if needed
output_video = "/content/shoplifting_dataset_video.mp4"

# Get sorted list of images
images = sorted([img for img in os.listdir(image_folder) if img.endswith(".jpg")])

if not images:
    print("❌ No images found! Check dataset path.")
else:
    # Read first image to get dimensions
    first_frame = cv2.imread(os.path.join(image_folder, images[0]))
    height, width, _ = first_frame.shape

    # Set up video writer
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    video = cv2.VideoWriter(output_video, fourcc, 5, (width, height))  # FPS = 5

    # Add images to video
    for img in images:
        frame = cv2.imread(os.path.join(image_folder, img))
        video.write(frame)

    video.release()
    print(f"✅ Video created: {output_video}")

!pip install ultralytics opencv-python

from ultralytics import YOLO

# Load YOLOv8 model
model = YOLO("yolov8n.pt")  # Use YOLOv8 nano

# Train the model
model.train(data="/content/dataset/data.yaml", epochs=100, batch=8, imgsz=640)

print("✅ Training Complete!")

import cv2

# Load trained YOLO model
model = YOLO("/content/runs/detect/train/weights")

# Set paths
video_path = "/content/shoplifting_dataset_video.mp4"
output_path = "/content/shoplifting_detection_output.mp4"

# Open video file
cap = cv2.VideoCapture(video_path)

import locale
locale.getpreferredencoding = lambda: "UTF-8"
!pip install ultralytics opencv-python

import cv2

# Load trained YOLO model
model = YOLO("/content/runs/detect/train/weights/best.pt")

# Set paths
video_path = "/content/shoplifting_dataset_video.mp4"
output_path = "/content/shoplifting_detection_output.mp4"

# Open video file
cap = cv2.VideoCapture(video_path)

# Get video properties
fourcc = cv2.VideoWriter_fourcc(*"mp4v")
fps = int(cap.get(cv2.CAP_PROP_FPS))
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

# Create VideoWriter
out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Run YOLO detection
    results = model(frame)

    # Draw bounding boxes
    for r in results:
        for box in r.boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            label = r.names[int(box.cls)]
            confidence = box.conf[0].item()

            if confidence > 0.5:  # Confidence threshold
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                cv2.putText(frame, f"{label} {confidence:.2f}", (x1, y1 - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # Write frame to output video
    out.write(frame)

cap.release()
out.release()

print(f"✅ Shoplifting detection complete! Output saved at: {output_path}")

import locale
locale.getpreferredencoding = lambda: "UTF-8"

import os
print(os.path.exists("/content/runs/detect/train/weights/"))

os.system("ls -l /content/runs/detect/train/weights/")

!ls -la "/content/runs/detect/train/weights/"

!find /content/runs/ -name "weights"

import os

labels_path = "/content/dataset/train/labels"  # Adjust path if needed
label_files = os.listdir(labels_path)

if label_files:
    print("✅ Labels found:", label_files[:5])  # Show first 5 labels
else:
    print("❌ No label files found! Check your dataset.")

from ultralytics import YOLO
import cv2

model = YOLO("/content/runs/detect/train/weights/best.pt")  # Load trained model
video_path = "/content/labeled_video.mp4"
cap = cv2.VideoCapture(video_path)

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    results = model(frame, conf=0.2)  # Reduce confidence threshold
    annotated_frame = results[0].plot()

    cv2.imshow("Shoplifting Detection", annotated_frame)
    if cv2.waitKey(1) & 0xFF == ord("q"):
        break

cap.release()
cv2.destroyAllWindows()

from google.colab.patches import cv2_imshow

from google.colab.patches import cv2_imshow  # Import necessary function
import os
import cv2
from ultralytics import YOLO

# Load trained YOLO model
model = YOLO("/content/runs/detect/train/weights/best.pt")  # Update to your model path

# Video path
video_path = "/content/shoplifting_dataset_video.mp4"  # Update to your video path

# Check if video file exists
if not os.path.exists(video_path):
    print("❌ Video file not found!")
else:
    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        print("❌ Failed to open video!")
    else:
        # Get FPS and frame dimensions
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        # Set up VideoWriter to save the output
        output_path = "/content/shoplifting_detection_output.mp4"
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use mp4v codec
        out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            # Run detection with adjusted confidence threshold and NMS
            results = model(frame, conf=0.5)  # Adjust confidence threshold as needed

            # Debug: Check number of detections
            print(f"Detected {len(results[0].boxes)} objects in this frame.")

            # Plot bounding boxes
            for box in results[0].boxes:
                x1, y1, x2, y2 = map(int, box.xyxy[0])
                label = results[0].names[int(box.cls)]
                confidence = box.conf[0].item()

                if confidence > 0.5:  # Confidence threshold
                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                    cv2.putText(frame, f"{label} {confidence:.2f}", (x1, y1 - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

            # Write the annotated frame to output video
            out.write(frame)

            # Display annotated frame using cv2_imshow()
            cv2_imshow(frame)

            # Option to break by pressing 'q'
            if cv2.waitKey(1) & 0xFF == ord("q"):
                break

        # Release resources
        cap.release()
        out.release()
        cv2.destroyAllWindows()

        print(f"✅ Output video saved at: {output_path}")

from google.colab.patches import cv2_imshow

"""For example, here we download and display a PNG image of the Colab logo:"""

import os
video_path = "/content/shoplifting_dataset_video.mp4"  # Replace with your actual path
print(os.path.exists(video_path))  # Should return True if the file exists

video_path = "/content/labeled_video.mp4"
print(os.listdir("/content"))  # Check if the video file is there

model = YOLO("/content/runs/detect/train/weights/best.pt")
# Assuming your video is in /content/
video_path = "/content/shoplifting_dataset_video.mp4"  # Change to the actual video path if needed
model.predict(source=video_path, conf=0.2, save=True)

import os
import cv2
from ultralytics import YOLO
import shutil
from sklearn.metrics import precision_score, recall_score, f1_score
import numpy as np

# Define paths
video_path = "/content/shoplifting_dataset_video.mp4"  # Path to input video
output_dir = "/content/runs/detect/predict"  # Output directory to save frames and video
output_video_path = "/content/shoplifting_detection_output.mp4"  # Path to save output video

# Load the trained YOLO model
model = YOLO("/content/runs/detect/train/weights/best.pt")

# Ensure the output directory exists and clear it
if os.path.exists(output_dir):
    shutil.rmtree(output_dir)  # Clear previous predictions
os.makedirs(output_dir)

# Run detection on video
results = model.predict(source=video_path, conf=0.2, save=True)

# Initialize variables for accuracy (if ground truth is available)
y_true = []  # Ground truth labels (if available)
y_pred = []  # Predicted labels

# Initialize video writer to create output video
cap = cv2.VideoCapture(video_path)
fps = cap.get(cv2.CAP_PROP_FPS)
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

fourcc = cv2.VideoWriter_fourcc(*"mp4v")  # Codec for video format
out_video = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))

frame_count = 0
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    frame_count += 1

    # Fetch the results for the current frame
    current_results = results[frame_count - 1]  # Adjust index if needed

    # Annotate frame with bounding boxes
    for result in current_results.boxes:
        x1, y1, x2, y2 = map(int, result.xyxy[0])
        label = current_results.names[int(result.cls)]
        confidence = result.conf[0].item()

        # Optionally, store predicted labels for accuracy
        y_pred.append(label)  # Save predicted label

        if confidence > 0.5:
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(frame, f"{label} {confidence:.2f}", (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # Write the annotated frame to output video
    out_video.write(frame)

    # Save frame as an image
    frame_img_path = os.path.join(output_dir, f"frame_{frame_count:04d}.jpg")
    cv2.imwrite(frame_img_path, frame)

cap.release()
out_video.release()

# Calculate precision, recall, F1-score (if ground truth labels available)
if y_true and y_pred:
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')

    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")
else:
    print("No ground truth labels provided. Skipping accuracy calculation.")

# Output paths
print(f"✅ Detection complete. Images saved to: {output_dir}")
print(f"✅ Output video saved to: {output_video_path}")

# Check training metrics after training
model = YOLO("/content/runs/detect/train/weights/best.pt")

# Validate the model to calculate metrics (if not already done during training)
results = model.val()  # Run validation to compute metrics

# Now you can access metrics using the box property
metrics = results.box

# Print the desired metrics from the 'box' property
print(f"Precision: {metrics.p[0]}")
print(f"Recall: {metrics.r[0]}")
print(f"mAP50-95: {metrics.map}")  # Access the scalar value directly
print(f"mAP50: {metrics.map50}")  # Access the scalar value directly



